Hadoop Interview questions

1.	Architecture of project.
2.	What is Hive.
3.	Are you incrementally appending the data or overwriting the data.
4.	Avoid duplicates while loading the data in hive

    insert overwrite table dynpart select distinct * from dynpart;

    Well, hive does not provide row level update/delete; therefore we can avoid the duplicate data while loading the data in base tables. 
As shown below
>CREATE TABLE RAW_TABLE  
(
    COL1 STRING,
    COL2 STRING,
    CREATEDATE STRING,
    DAYID STRING,
    MARKETID STRING
)
ROW FORMAT DELIMITED 
FIELDS TERMINATE BY'\t'
STORED AS TEXTFILE;

>LOAD DATA INPATH '/FOLDER/TO/EXAMPLE.txt  INTO RAW_TABLE;

>CREATE TABLE JLT_CLEAN AS
SELECT col1,
  col2,
  dayid,
  marketid,
  MAX(createdate) AS createdate
FROM JLT_STAHING
GROUP BY col1,
  col2,
  dayid,
  marketid;

5.	Difference between bucketing and partitioning
Partitioning
•	Partitioning is used to divide the table into different partitions. Each partition is stored as a different directory.
•	A partition is created for each unique value of the partition column.
•	Hierarchical partitioning can be done by specifying the partitioning columns in a sequence as per the hierarchy like Country, State, City.
•	We cannot control the number of partitions if the value of partitioning columns have a very high cardinality.
•	Partitioning allows hive to avoid full table scan if partition columns are used in the where clause of hive query. A query containing partition columns in the where clause will scan directories for specific partition only.
Bucketing
•	Bucketing is used to distribute/organize the data into fixed number of buckets.
•	Each bucket is stored as a file under the Table/Partition directory.
•	The number of buckets are fixed at the table creation time. All the data will be distributed into these buckets based on the hash value of the bucketing columns.
•	Which records go to which bucket are decided by the Hash value of columns used for bucketing.
•	A Bucket will have all the records for same value of bucketing columns.
•	A Bucket will have all the records for same Hash value of bucketing columns. So records having different value of bucketing columns but having same hash value will go into the same bucket.
•	Bucketing is used for efficient map-side joins between bucketed tables and for effectively executing sampling queries.
 

6.	Difference between order by, sort by, distributed by and clustered by.
? ORDER BY x: guarantees global ordering, but does this by pushing all data through just one reducer. This is basically unacceptable for large datasets. You end up one sorted file as output.
? SORT BY x: orders data at each of N reducers, but each reducer can receive overlapping ranges of data. You end up with N or more sorted files with overlapping ranges.
? DISTRIBUTE BY x: ensures each of N reducers gets non-overlapping ranges of x, but doesn't sort the output of each reducer. You end up with N or unsorted files with non-overlapping ranges.
? CLUSTER BY x: (Combination of SORT BY and DISTRIBUTED by) ensures each of N reducers gets non-overlapping ranges, and then sorts by those ranges at the reducers. This gives you global ordering, and is the same as doing (DISTRIBUTE BY x and SORT BY x). You end up with N or more sorted files with non-overlapping ranges.

7.	When to go for sort by and order by
8.	What is difference between like and rlike.
Performs a pattern match of a string expression expr against a pattern pat. The pattern can be an extended regular expression. “Regular Expressions”. Returns 1 if expr matches pat; otherwise it returns 0. If either expr or pat is NULL, the result is NULL. RLIKE is a synonym for REGEXP, provided for mSQL compatibility. 

Pattern matching using SQL simple regular expression comparison. Returns 1 (TRUE) or 0 (FALSE). If either expr or pat is NULL, the result is NULL. 

9.	Have you used CASE statement in HIVE. If yes is it possible to use default along with case statement in HIVE.
10.	Is it possible to UNION between two tables having different number of records?
11.	What is LEAD and LAG function in hive.
12.	Write the sqoop statement for inserting data from RDBMS to Hadoop.
    Sqoop import –connect jdbc:mysql://localhost/test –username cloudera –password cloudera –hive-import –hive-table –target-dir /emp/ --m 8
13.	What are the file formats that are used in your project in HIVE?
    ORC for optimization 
14.	Write the mapreduce code for wordcount.
15.	How many node cluster you are working with in your project.
16.	How to find the top 3 sal from emp table using hive.
    select * from (select salary, ROW_NUMBER() over (ORDER BY salary) as row_no from emp group by salary) res where res.row_no = 4

17.	How to 8th sal from emp by using sql.
18.	What is indexing
19.	What are the UDF’s in HIVE
20.	How many mappers and reducers will run when you execute following queries.
Select * from emp.—No mapper(selecting all columns, not filtering any column)  and no reducer(since there is no aggregations are happening)
Select count(*) from emp;--  mapping and Reducer will be invoked since there is only aggregation is happening 
Select Dept, count(dept) from emp group by dept;-- here both map and reduce phases comes since both aggregation and filtered columns
Select * from emp where id=10; --Only map phase will run since there are no aggregation is happening   
21.	What is skewed join, replicated join.
22.	What is mapside join.
23.	When to go for mapside join and reducer side join
24.	What is the difference between comparator and comparable
25.	Difference between abstract and interface
26.	Difference between hashtable and hashmap
27.	What is synchronized mean in java.
28.	Difference between list and set
29.	How to achieve multiple inheritance.
30.	What is RANK and DENSERANK
31.	What is the default number of mappers in sqoop and what if we give 10 mappers and how it will impact the performance
32.	What is a partitioner in mapreduce
33.	What is Cassandra
34.	How to schedule jobs in oozie
35.	What if we don’t mention schema in pig while loading data
36.	What is the query to load data into partitioned table and why should we have to mention the partition column in the end of select statement while inserting data
37.	What is the difference between external and managed table and when to go for them
38.	When to go for static and dynamic partitioning
39.	What are the file formats supported in hive and what is ORC.
40.	 What is SerDE in hive.
41.	How to find the 7th highest value in array in java.
42.	How to pass param file in sqoop.
43. How to get particular row from a file?       

https://stackoverflow.com/questions/40204001/how-does-mapreduce-recover-from-errors-if-failure-happens-in-an-intermediate-sta


1. How to import the data from rdbms to hadoop using sqoop for 10 tables out of 100 tables in job
2. How many tables u have worked on for sqoop importing
3.How to write a UDF in pig and how to register it in case the class has many functions in it
4. Difference between cogroup n group in pig
    The GROUP and COGROUP operators are identical. Both operators work with one or more relations. 
    For readability GROUP is used in statements involving one relation and COGROUP is used in statements involving two or more relations. 
    You can COGROUP up to but no more than 127 relations at a time.
5. How to find max sal of a dept in pig
    A = load data 'a.txt' using PigStorage(',') (id:int, name:chararray,salary:int, Dept:int)
    B = Group A by A.dept;
    C = foreach B generate group,max(A.sal);
dump C;
6. Based on what how many buckets we have to decide?
    One thing buckets are used for is to increase load performance
    SELECT performance ( predicate pushdown )
    Join Performance ( bucket join )
    Some sample operations can get faster with buckets.

7. What will happen when we import data to a dynamic partition table if source table is empty
    Since there is no data, no partition will be created 
8. What is default partitioning in hive?
    When you define a table in Hive with a partitioning column of type STRING, all NULL values within the partitioning column appear as __HIVE_DEFAULT_PARTITION__ in the output of a SELECT from Hive statement
9. What is serialisation?
    Serialization in Java is a mechanism of writing the state of an object into a byte stream. It is mainly used in Hibernate, RMI, JPA, EJB and JMS technologies. The reverse operation of serialization is called deserialization.

10. What is difference between arraylist n linkedlist?
11. What is hdfs archiving?
    The HDFS Archival Storage feature gives you the option to reduce cost by keeping rarely used data on less expensive but higher  density storage. Such data is usually referred to as “cold” data. Unlike traditional data archives, with archival storage on the HDFS, the data is still accessible. Applications can easily retrieve and process such data.
12. How to merge data in hive?
    By using --merge-key for normal import
    sqoop import --connect jdbc:mysql://localhost/test --table emp \
                 --username hive -password hive --incremental lastmodified --merge-key empid --check-column cr_date \
                 --target-dir /sqoop/empdata/
    By using --merge-key for hive import
    sqoop merge --new-data /apps/hive/warehouse/student/part-m-00000
                --onto /apps/hive/warehouse/student/part-m-00000_copy_1
                --target-dir /tmp/sqoop_merge
                --jar-file /tmp/sqoop-ambari-qa/compile/9062c87c959e4090dcec5995a439b514/TIME.jar
                --class-name TIME
                --merge-key TIME
  
13. What is S3 in AWS?
    

14. What is cloudwatch in AWS (scheduling)?
    Amazon CloudWatch allows developers, system architects, and administrators to monitor their AWS applications in the cloud, in near-real-time. CloudWatch is automatically configured to provide metrics on request counts, latency, and CPU usage. Users also can send their own logs and custom metrics to CloudWatch for monitoring.
    The data and reports CloudWatch provides lets users keep track of application performance, resource use, operational issues, and constraints. This helps organizations resolve technical issues and streamline operations.
    CloudWatch is most commonly used with Elastic Compute Cloud (EC2) instances, and can also monitor Amazon Elastic Block Store (EBS) volumes, Elastic Load Balancers (ELBs), and instances of Amazon Relational Database Service (RDS). It also can extend beyond these core services to intake custom data from external sources.

Users choose CloudWatch for its automatic integration with AWS services, its flexibility, and its ability to scale quickly.
15. What is transient and volatile data members in java?
    By making a variable or field transient in a Class prevents it from being Serialized in Java.  Along with static variables, transient variables are not serialized during Serialization and they are initialized with there default value during deserialization process e.g. an int transient variable is initialized with zero during deserialization in Java. to learn more about transient variable, See What is transient variable in Java.
    On the other hand volatile variables are used in Concurrent programming in Java. When we declare a variable volatile,  every thread reads its value from main memory and don't used cached value available in every thread stack. volatile variable also prevents compiler from doing reordering which can compromise synchronization. to learn more about volatile variables, read What is volatile variable in Java.  Many Java programmer though know about volatile variables they are not sure where to use volatile modifier in Java. One of the popular example of  using volatile variable is implementing double checked locking in Singleton, where Singleton instance of class is declared volatile in Java.
    
16. What is final method in java?
    A final method cannot be overridden. Which means even though a sub class can call the final method of parent class without any issues but it cannot override it.
17. Can abstract class be made final in java?
    No, an abstract class cannot be final, Abstract methods needs to be implemented; therefore it is overridden in the sub class. But by marking final, we are restricting it to override. Compile-time error will be thrown: Error message: The abstract method display in type <abstract-class-name> can only set a visibility modifier, one of public or protected
18. What are checked n unchecked exceptions?
    Checked exceptions are the exceptions that are checked at compile time. If some code within a method throws a checked exception, then the method must either handle the exception or it must specify the exception using throws keyword.
    Unchecked are the exceptions that are not checked at compiled time. In Java exceptions under Error and RuntimeException classes are unchecked exceptions, everything else under throwable is checked.

DS:
1. Spark interview Questions with Answers 
http://bigdatascholars.blogspot.com/2018/07/spark-interview-questions.html
2. Bigdata Interview Questions with Answers
http://bigdatascholars.blogspot.com/2018/07/bigdata-interview-questions.html
3.Hive Interview Questions with Answers
http://bigdatascholars.blogspot.com/2018/08/hive-interview-questions-with-answers.html
4.SQOOP Interview Questions with Answers
http://bigdatascholars.blogspot.com/2018/08/sqoop-interview-question-and-answers.html

IBM interview questions
1. if a Job gets fail how will you handle it for example connectivity issues
    We have to perform the cleanup activity corresponding to the job and then rerun the job
2. What format you used in your project? why avro is not used although it gives more compression
3. If you have 100 tables how will you write the script for creating table? Are you writing individually or using Utility?
4. How will you make sure if the data is loaded properly or not using sqoop
5. how are scheduling your sqoop jobs? any utility?
6. how will you write that oozie XML, can you explain briefly and what parameters did you use in that XML
7. Suppose there is a table which got loaded into hive later they have changed the schema in RDBMS like they have removed the columns, so how are you going to load the data? Any checkup are you doing?
    In this case Job will not fail first and if they remove any column from RDBMS, corresponding values will be inserted as NULL further     in HIVE table
8. Suppose there is a table which got loaded into hive later they have changed the schema in RDBMS like they have added the columns, so how are you going to load the data? Any checkup are you doing?
    Incase if they add any column from RDBMS, those values will be loaded into part-m file but only displays the data correspoding to       columns which exist in HIVE, So we need to add the column manually in HIVE, so that it displays properly.
    
9. where exactly you are using Pig in your project, whatever you are doing same thing can be done in hive? so why to for Pig
10. How to update a specific data in hive? Since it does not supported updating hive
11. How are you validating the datatypes between RDBMS and HIVE
    Automatically the RDBMS datatypes will be converted to HIVE datatypes. for example VARCHAR will be converted as a string 
    But not sure about validation.


Capgemini:
1. Kinds of tables in hive
2. select empid from empgroup by empid having count(*)>1, what is will be the result of this query?
    Duplicate empid's will be displayed
3. How to get 1st,5th, 10th,29th.30th lines from a Log file?
4. Can we perform a Join operation where you dont have common data between tables
5. Logic for transpose of a row 
6. If we want to get the row specific line number from log file, what will you do?( I said grep but I knew its wrong)
7. Can we move the logs file using sqoop?if not why?(I said no it's not possible)
8. If comparable is there why to go for comparator something he was asking
